<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Anti-fragility in (Distributed) Systems | Ankil Patel</title>
<meta name=keywords content>
<meta name=description content="Motivation I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn&rsquo;t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems?">
<meta name=author content="Ankil">
<link rel=canonical href=https://ankilp.github.io/posts/ai-primitives/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.6cba0d81b5f3f42bb578d49f402ba4175aa72b43def148780b8ad714c957c6f5.css integrity="sha256-bLoNgbXz9Cu1eNSfQCukF1qnK0Pe8Uh4C4rXFMlXxvU=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ankilp.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://ankilp.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://ankilp.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://ankilp.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://ankilp.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.2">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YXFJCKLYLT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-YXFJCKLYLT',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Anti-fragility in (Distributed) Systems">
<meta property="og:description" content="Motivation I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn&rsquo;t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems?">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ankilp.github.io/posts/ai-primitives/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2023-08-30T00:00:00+00:00">
<meta property="article:modified_time" content="2023-08-30T00:00:00+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Anti-fragility in (Distributed) Systems">
<meta name=twitter:description content="Motivation I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn&rsquo;t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems?">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://ankilp.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Anti-fragility in (Distributed) Systems","item":"https://ankilp.github.io/posts/ai-primitives/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Anti-fragility in (Distributed) Systems","name":"Anti-fragility in (Distributed) Systems","description":"Motivation I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn\u0026rsquo;t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems?","keywords":[],"articleBody":"Motivation I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn’t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems? AI provides an avenue for creating agents that can adapt to new information and learn more efficiently than previous techniques.\nA part of me wondered how many of the system-level issues we saw came from insufficient designs, changing environments that were no longer suitable to the implementation, terrible metrics (or lack thereof) and intuitive decision-making vs data-driven processes. Unfortunately, most of these problems are endemic of the entire industry, partly because of inertia, lack of infrastructure and de-prioritization because these issues aren’t visible to customers.\nWhen I search for AI in distributed systems or AI in large scale systems, I see articles, papers and blogs on how to create systems that can train or do inference - i.e. create systems that can help create models. But I haven’t seen many instances on how to use AI to boost the performance of end-to-end systems in production. Jane Street, Databricks and Google have invested in using AI in their infrastructure but I largely see this idea discussed in academia suggesting to me that this idea has enough merit to investigate and potentially enough that companies are willing to put their weight behind it. Note, I’m not talking about augmentation of distributed systems, e.g. forecasting of traffic to provide signals to an autoscaler but rather I’m talking about something more in the vein of Software 2.0.\nSoftware 2.0 focuses on the individual services while I’m proposing a different approach for building systems by focusing on system aspects like antifragility. My first thought was to create an ‘AI’ agent to adjust parameters on a config file. A config file has very specific assumptions about the system, the types and the format. However, it’s not an intrinsic part of the system, it’s just an interface to the system. It’s easier to sell and integrate into tech stacks, but we need to make assumptions about the behaviour of the underlying service and integrity of the interface (e.g. fields in the config that get ignored in the code). This suggests to me that config files are really second class system primitives. The distinction between a first class and second class primitive is that a first class primitive is an intrinsic aspect of the system and can evolve with the system. An interface to the system is more of a proxy and it requires collaboration between teams and considerably more re-training. Moreover, it can involve shims and careful engineering work to migrate to new versions.\nI understand why it’s been designed this way: classical control theory has rigorously proven theorems and established methods of achieving stability and correctness - hence why kubernetes is still using controllers invented in 1900s. When I asked my reinforcement learning (RL) professor what the difference was between reinforcement learning and control theory: his response was that the two research groups just don’t meet despite solving very similar problems. That seems like a missed opportunity. My aim is to replace some of these ‘control theory’ controllers with ‘reinforcement learning’ controllers because we’ve seen how compact and simple RL controllers can be while emulating the same behaviour.\nFor a business serving customers, stability and correctness translate directly to customer retention. However, these needs have changed and even startups can start to ingest thousands of GBs of data every day, serve thousands of customers within a relatively short period of time, all while battling cloud costs in the name of growth. On top of that, when systems inevitably break because some of config issues, unexpected traffic distributions, latent system bottlenecks, etc. we tend to assign multiple engineers to fix those issues, costing the business money and, what is infinitely more valuable, time. If our needs and capabilities have changed, why haven’t the systems underpinning the modern internet changed?\nWhy must we limit ourselves to such rigid system primitives? Instead, what if we had differentiable, tunable and intrinsic system primitives that were exposed to a learning agent. Chaos testing is to systems as what sanitizers are to C++ in that both are afterthoughts. Why can’t we build systems that intrinsically exploit stressors to adapt to them?\nHierarchal Autonomous Agents I’ve hinted throughout this blog post that we could realize global efficiencies across the entire system by having individual AI primitives coordinate with a hierarchal AI agent that could optimize objectives spanning across the entire system. We have effectively created a multi-agent system where the agents must cooperate to realize their common goals.\nFor brevity, I will call this top-level AI agent: central controller (CC).\nI’d like to motivate the need for the CC with an example exemplifying the system level changes that are incurred when a new endpoint is added. The new endpoint is serving a small group of customers initially who are read-heavy. Maybe we should re-shard so that we can handle the new number of read requests? Or maybe instead we should create a cache in front of the database and schedule the polling of the database to validate the cache based on historical data of when the database isn’t used as much? Or maybe the current system can handle the traffic but the system can exploit database caches if it ensures that the read requests from the new endpoint are issued from the same nodes that the write requests are assigned to? How do we decide which of these solutions is globally better? How do we change these ideas What objectives are driving decisions at organizations: money saved? strategic positioning? reducing complexity? Eventually, it’d be great to be able to train joint text-policy embeddings so we can ask the CC, in natural language, to optimize on a set of objectives - yet another use case that accessible LLMs have made possible.\nIt’s open to interpretation whether we want the CC to directly design other agents' policies.\nSystem/AI Primitives Generally, a lot of system-level problems can be framed as some optimization problem and I’m going to explore what AI primitives are by introducing a few examples that solve assignment, scheduling and search problems. Note that these are just a subset of what we can build.\nAssignment Problems Load Balancing The goal of load balancing is to make sure that we distribute requests, jobs and workloads to all available server, workers and buckets in a way that doesn’t overwhelm any one bucket. We can do this with consistent hashing or rendezvous hashing if we expect any of the nodes to experience downtime but the basic idea is to facilitate some sort of an assignment problem. These aren’t cryptographic hash functions so we can use a hash function that’s parallelizable and pipeline-able, so long as we can bound entropy on each bucket and on the collective. However, what if I ask you to implement anti-affinity, affinity, sparsity and capacity (e.g. heterogeneous compute)? We want to embed all that information and other biases into the hash function to direct requests to certain nodes. But what if we can use empirical data to tune the hash function based on short- and long-term trends. Even large companies like Ali Baba (I can’t find the article but they describe running into hot spotting issues during Christmas despite designing a resilient system) have had issues during Black Swan events.\nThere are ideas from image retrieval or general kernel-based hashing on creating kernels that can embed that information. The basic idea is that you have a vector space of load balancing configurations where each vector corresponds to a hashing function. For this specific load balancing problem, instead of minimizing entropy during training, we can maximize entropy - e.g. images/requests that are more unlike each other are placed in the same bucket. We can create hash functions from these kernels and use them with consistent or rendezvous hashing.\nThe real benefits of the system only become clearer with a holistic view of the system (section coming later) - you can redirect requests based on how downstream services are tuning their traffic shaping, how backed up downstream queues are or which versions are being deployed. This thinking can expand to the entire network topology. What if we could have an AI agent tuning this kernel, and consequently redirecting requests or jobs, based on dynamic shifts in traffic and system health across the organization?\nThis differentiable kernel can be interpreted as a system primitive (embedding) in a configuration space. Why is it important that it is differentiable? So we can take feedback from various signals and tune the kernel in an efficient way and because non-differentiable functions are harder to optimize. Kernels are just one interpretation of AI primitives. We can think of this assignment problem more abstractly: it could also be a RL agent that learns how to send requests to the right node depending on system characteristics. And the policy that it learns from interacting with the system can be a system primitive. We can create hierarchal RL agents that talk to each other and optimize different parts or dimensions of the system as a whole (section below).\nSuppose we have a hundred load balancers, each with its own set of parameters, traffic distributions and environments. And now, I come to you and say I want another load balancer but I don’t want to train this hash function/RL agent/primitive again. Well, what if we transport all these embeddings into a super embedding space that’s agnostic to implementation details. Then, we can query this super configuration space for another load balancer primitive given some parameters and then run it through a decoder network to get something implementation specific. The great part about this super embedding space is that it can be reused across organizations, companies and scales.\nSharding Sharding relocates certain rows and columns in a database to make it easier to scale read and write workloads. However, with more complex data queries which include joins, database caches, external caches sitting on top of databases, and frequency of changing values and quantities, sharding becomes a tough optimization problem. For example, if there’s a thundering herd targeting a specific group of shards, we might want to be able to reorganize shards to be capable of addressing those scaling issues while we’re serving users and without manual intervention.\nLike load balancing, sharding is also an assignment problem. However, it differs from load balancing in how the underlying model is trained. Re-sharding can be very expensive so we would want to bound the deviation in the kernel and include the cost of re-sharding in its cost function. Moreover, the frequency at which we change parameters would be drastically different from how we might change the load balancer.\nThe key on which we shard also depends on how we’re using these databases. A localized view of stressors might create a tuning that doesn’t address long-term trends. And it doesn’t account for human intuition about human-centric traditions like Christmas-time load. An empirical model trained with 11 months of data with slight disruptions might never understand why the system becomes very busy on the 12th month. Or it might consider the 12th month as a anomaly.\nLog LLM(?) That brings me to log databases - can we query system history to make informed decisions about how to bias the sharding strategy? This log database is very important for sharding because re-sharding can be very expensive. Can this system history be used to tune primitives and allow them to anticipate Christmas-time load while also learning and handling day-to-day loads?\nWhat if we could intelligently query system logs so that\n primitives can actively tune themselves based on immediate stressors and historical data that’s been filtered so only relevant information comes up we can ask where the system failed before and recreate those scenarios to test primitives, i.e. hypothesis testing  Currently, I see a few tools that are capable of doing this with varying levels of success and objectives. Structured is a new company created to address this issue but entrenched players are also stepping in capture this market.\nSearch Problems Query Optimizers When queries aren’t optimized correctly, it can introduce delays across the system because databases underpin a lot of systems. Solutions like airops and postgres.ai already exist but they’re not first class primitives. They’re interfaces to the query optimizer, which need to be tediously maintained, especially as the system’s execution and storage engines evolve. There are benefits to understanding the history of calls to a database to exploit caches and improve index selection. What does a first class query optimizer primitive look like?\nNeo proposes a neural network based query optimizer that is comparable to optimizers from Microsoft and IBM. More importantly, it can dynamically adapt to incoming queries, building upon its successes and learning from its failure. Does this neural network fit the model of an system primitive? By adjusting the weights of this neural network, we can address novel situations based on the environment, storage needs, and traffic distribution.\nSystem primitives can come in various shapes and formats - the invariance is that they can learn. I will add that they can also communicate and interact with an AI agent (section coming later).\nScheduling Problems Schedulers So far, we’ve talked about creating first class system primitives for assignment and search problems. What about scheduling problems? What about scheduling and assignment problems? When you submit jobs to your cluster, you request system resources like compute and memory. But the system doesn’t precisely know how much compute and memory you’ll need in the duration of your program execution. What happens if there’s a large burst of ML training jobs and you need to quickly provision compute and memory for those jobs? You could overprovision and waste resources or underprovision and redo or slow down the task. There are a few examples that have already used AI to find a better packing solution.\nWe also see scheduling problems in process schedulers for hyper-V. We’ve already seen RL agents handle generic scheduling and assignment problems and do considerably better than conventional optimization techniques, suggesting that we could realize significant gains in many aspects of the system given how ubiquitous the scheduling+assignment problem is in systems.\nDeployments How do we deploy a system like this? How can we as humans interpret outcomes? How can we debug these solutions? It’s not as straightforward to debug a declarative system. But there are more established methods to productionize AI models like Claypot that at least create precedence for how to design these systems, ensure that we can detect model drift (among many other factors) and rollback when the model quality starts slipping.\n If you’re interested, I’m in the process of writing another blog post on why we need another compiler for distributed compute that addresses both mobility of code and locality of resources. I’m not convinced that present compilers understand system requirements - they’re locally optimized for the code but not globally optimized for the system. Since moving to the states, I’ve noticed a general trend of fragmented systems: health, medical, finance, banking… There’s a blog coming up on this too.\n","wordCount":"2567","inLanguage":"en","datePublished":"2023-08-30T00:00:00Z","dateModified":"2023-08-30T00:00:00Z","author":{"@type":"Person","name":"Ankil"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ankilp.github.io/posts/ai-primitives/"},"publisher":{"@type":"Organization","name":"Ankil Patel","logo":{"@type":"ImageObject","url":"https://ankilp.github.io/favicon.ico"}}}</script>
</head>
<body class=dark id=top><head>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YXFJCKLYLT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-YXFJCKLYLT',{anonymize_ip:!1})}</script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','G-YXFJCKLYLT','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://ankilp.github.io accesskey=h title="Ankil Patel (Alt + H)">Ankil Patel</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://ankilp.github.io>Home</a>&nbsp;»&nbsp;<a href=https://ankilp.github.io/posts/>Posts</a></div>
<h1 class=post-title>
Anti-fragility in (Distributed) Systems
</h1>
<div class=post-meta>August 30, 2023&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Ankil
</div>
</header>
<div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2>
<p>I asked a former manager (I have not asked him yet if I can use his name in this blog post) about how to build resilient systems and his response was that I had the wrong idea. The holy grail wasn&rsquo;t in resilient systems - it was anti fragile systems (from the book by Taleb). Anti-fragile systems are able to learn and become better with new stressors. How do we build antifragile systems? AI provides an avenue for creating agents that can adapt to new information and learn more efficiently than previous techniques.</p>
<p>A part of me wondered how many of the system-level issues we saw came from insufficient designs, changing environments that were no longer suitable to the implementation, terrible metrics (or lack thereof) and intuitive decision-making vs data-driven processes. Unfortunately, most of these problems are endemic of the entire industry, partly because of inertia, lack of infrastructure and de-prioritization because these issues aren&rsquo;t visible to customers.</p>
<p>When I search for AI in distributed systems or AI in large scale systems, I see <a href=https://engineering.fb.com/2021/07/15/open-source/fsdp/>articles</a>, papers and blogs on how to create systems that can train or do inference - i.e. create systems that can help create models. But I haven&rsquo;t seen many instances on how to use AI to boost the performance of end-to-end systems in production. <a href=https://blog.janestreet.com/what-the-interns-have-wrought-2020/>Jane Street</a>, Databricks and <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview>Google</a> have invested in using AI in their infrastructure but I largely see this idea discussed in academia suggesting to me that this idea has enough merit to investigate and potentially enough that companies are willing to put their weight behind it. Note, I&rsquo;m not talking about augmentation of distributed systems, e.g. forecasting of traffic to provide signals to an autoscaler but rather I&rsquo;m talking about something more in the vein of <a href=https://karpathy.medium.com/software-2-0-a64152b37c35>Software 2.0</a>.</p>
<p>Software 2.0 focuses on the individual services while I&rsquo;m proposing a different approach for building systems by focusing on system aspects like antifragility. My first thought was to create an &lsquo;AI&rsquo; agent to adjust parameters on a config file. A config file has very specific assumptions about the system, the types and the format. However, it&rsquo;s not an intrinsic part of the system, it&rsquo;s just an interface to the system. It&rsquo;s easier to sell and integrate into tech stacks, but we need to make assumptions about the behaviour of the underlying service and integrity of the interface (e.g. fields in the config that get ignored in the code). This suggests to me that config files are really second class system primitives. The distinction between a first class and second class primitive is that a first class primitive is an intrinsic aspect of the system and <em>can evolve with the system</em>. An interface to the system is more of a proxy and it requires collaboration between teams and considerably more re-training. Moreover, it can involve shims and careful engineering work to migrate to new versions.</p>
<p>I understand why it&rsquo;s been designed this way: classical control theory has rigorously proven theorems and established methods of achieving stability and correctness - hence why kubernetes is still using controllers invented in 1900s. When I asked my reinforcement learning (RL) professor what the difference was between reinforcement learning and control theory: his response was that the two research groups just don&rsquo;t meet despite solving very similar problems. That seems like a missed opportunity. My aim is to replace some of these &lsquo;control theory&rsquo; controllers with &lsquo;reinforcement learning&rsquo; controllers because we&rsquo;ve seen how compact and simple <a href=https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control>RL controllers</a> can be while emulating the same behaviour.</p>
<p>For a business serving customers, stability and correctness translate directly to customer retention. However, these needs have changed and even startups can start to ingest thousands of GBs of data every day, serve thousands of customers within a relatively short period of time, all while battling cloud costs in the name of growth. On top of that, when systems inevitably break because some of config issues, unexpected traffic distributions, latent system bottlenecks, etc. we tend to assign multiple engineers to fix those issues, costing the business money and, what is infinitely more valuable, time. If our needs and capabilities have changed, why haven&rsquo;t the systems underpinning the modern internet changed?</p>
<p>Why must we limit ourselves to such rigid system primitives? Instead, what if we had differentiable, tunable and <em>intrinsic</em> system primitives that were exposed to a learning agent. Chaos testing is to systems as what sanitizers are to C++ in that both are afterthoughts. Why can&rsquo;t we build systems that intrinsically exploit stressors to adapt to them?</p>
<h2 id=hierarchal-autonomous-agents>Hierarchal Autonomous Agents<a hidden class=anchor aria-hidden=true href=#hierarchal-autonomous-agents>#</a></h2>
<p>I&rsquo;ve hinted throughout this blog post that we could realize global efficiencies across the entire system by having individual AI primitives coordinate with a hierarchal AI agent that could optimize objectives spanning across the entire system. We have effectively created a <a href=https://arxiv.org/pdf/1911.10635.pdf>multi-agent</a> system where the agents must cooperate to realize their common goals.</p>
<p>For brevity, I will call this top-level AI agent: central controller (CC).</p>
<p>I&rsquo;d like to motivate the need for the CC with an example exemplifying the system level changes that are incurred when a new endpoint is added. The new endpoint is serving a small group of customers initially who are read-heavy. Maybe we should re-shard so that we can handle the new number of read requests? Or maybe instead we should create a cache in front of the database and schedule the polling of the database to validate the cache based on historical data of when the database isn&rsquo;t used as much? Or maybe the current system can handle the traffic but the system can exploit database caches if it ensures that the read requests from the new endpoint are issued from the same nodes that the write requests are assigned to? How do we decide which of these solutions is globally better? How do we change these ideas What objectives are driving decisions at organizations: money saved? strategic positioning? reducing complexity? Eventually, it&rsquo;d be great to be able to train joint text-policy embeddings so we can ask the CC, in natural language, to optimize on a set of objectives - yet another use case that accessible LLMs have made possible.</p>
<p>It&rsquo;s open to interpretation whether we want the CC to directly design other agents' policies.</p>
<h2 id=systemai-primitives>System/AI Primitives<a hidden class=anchor aria-hidden=true href=#systemai-primitives>#</a></h2>
<p>Generally, a lot of system-level problems can be framed as some optimization problem and I&rsquo;m going to explore what AI primitives are by introducing a few examples that solve assignment, scheduling and search problems. Note that these are just a subset of what we can build.</p>
<h2 id=assignment-problems>Assignment Problems<a hidden class=anchor aria-hidden=true href=#assignment-problems>#</a></h2>
<h3 id=load-balancing>Load Balancing<a hidden class=anchor aria-hidden=true href=#load-balancing>#</a></h3>
<p>The goal of load balancing is to make sure that we distribute requests, jobs and workloads to all available server, workers and buckets in a way that doesn&rsquo;t overwhelm any one bucket. We can do this with consistent hashing or rendezvous hashing if we expect any of the nodes to experience downtime but the <strong>basic idea is to facilitate some sort of an assignment problem</strong>. These aren&rsquo;t cryptographic hash functions so we can use a hash function that&rsquo;s parallelizable and pipeline-able, so long as we can bound entropy on each bucket and on the collective. However, what if I ask you to implement anti-affinity, affinity, sparsity and capacity (e.g. heterogeneous compute)? We want to embed all that information and other biases into the hash function to direct requests to certain nodes. But what if we can use empirical data to tune the hash function based on short- and long-term trends. Even large companies like Ali Baba (I can&rsquo;t find the article but they describe running into hot spotting issues during Christmas despite designing a <a href=https://note.com/kenwagatsuma/n/n33f1caaab7cd>resilient system</a>) have had issues during Black Swan events.</p>
<p>There are ideas from <a href=https://arxiv.org/pdf/2101.11282.pdf>image retrieval</a> or general <a href=https://ieeexplore.ieee.org/document/6247912>kernel-based hashing</a> on creating kernels that can embed that information. The basic idea is that you have a vector space of load balancing configurations where each vector corresponds to a hashing function. For this specific load balancing problem, instead of minimizing entropy during training, we can maximize entropy - e.g. images/requests that are more unlike each other are placed in the same bucket. We can create hash functions from these kernels and use them with consistent or rendezvous hashing.</p>
<p>The real benefits of the system only become clearer with a holistic view of the system (section coming later) - you can redirect requests based on how downstream services are tuning their traffic shaping, how backed up downstream queues are or which versions are being deployed. This thinking can expand to the entire network topology. What if we could have an AI agent tuning this kernel, and consequently redirecting requests or jobs, based on dynamic shifts in traffic and system health across the organization?</p>
<p>This differentiable kernel can be interpreted as a system primitive (embedding) in a configuration space. Why is it important that it is differentiable? So we can take feedback from various signals and tune the kernel in an efficient way and because non-differentiable functions are harder to optimize. Kernels are just one interpretation of AI primitives. We can think of this assignment problem more abstractly: it could also be a RL agent that learns how to send requests to the right node depending on system characteristics. And the policy that it learns from interacting with the system can be a system primitive. We can create hierarchal RL agents that talk to each other and optimize different parts or dimensions of the system as a whole (section below).</p>
<p>Suppose we have a hundred load balancers, each with its own set of parameters, traffic distributions and environments. And now, I come to you and say I want another load balancer but I don&rsquo;t want to train this hash function/RL agent/primitive again. Well, what if we transport all these embeddings into a <a href=https://neurips.cc/virtual/2021/workshop/21833>super embedding space</a> that&rsquo;s agnostic to implementation details. Then, we can query this super configuration space for another load balancer primitive given some parameters and then run it through a decoder network to get something implementation specific. The great part about this super embedding space is that it can be reused across organizations, companies and scales.</p>
<h3 id=sharding>Sharding<a hidden class=anchor aria-hidden=true href=#sharding>#</a></h3>
<p>Sharding relocates certain rows and columns in a database to make it easier to scale read and write workloads. However, with more complex data queries which include joins, database caches, external caches sitting on top of databases, and frequency of changing values and quantities, sharding becomes a tough optimization problem. For example, if there&rsquo;s a thundering herd targeting a specific group of shards, we might want to be able to reorganize shards to be capable of addressing those scaling issues while we&rsquo;re serving users and without manual intervention.</p>
<p>Like load balancing, sharding is also an assignment problem. However, it differs from load balancing in how the underlying model is trained. Re-sharding can be very expensive so we would want to bound the deviation in the kernel and include the cost of re-sharding in its cost function. Moreover, the frequency at which we change parameters would be drastically different from how we might change the load balancer.</p>
<p>The key on which we shard also depends on how we&rsquo;re using these databases. A localized view of stressors might create a tuning that doesn&rsquo;t address long-term trends. And it doesn&rsquo;t account for human intuition about human-centric traditions like Christmas-time load. An empirical model trained with 11 months of data with slight disruptions might never understand why the system becomes very busy on the 12th month. Or it might consider the 12th month as a anomaly.</p>
<h4 id=log-llm>Log LLM(?)<a hidden class=anchor aria-hidden=true href=#log-llm>#</a></h4>
<p>That brings me to log databases - can we query system history to make informed decisions about how to bias the sharding strategy? This log database is very important for sharding because re-sharding can be very expensive. Can this system history be used to tune primitives and allow them to anticipate Christmas-time load while also learning and handling day-to-day loads?</p>
<p>What if we could intelligently query system logs so that</p>
<ol>
<li>primitives can actively tune themselves based on immediate stressors and <em>historical data</em> that&rsquo;s been filtered so only relevant information comes up</li>
<li>we can ask where the system failed before and recreate those scenarios to test primitives, i.e. hypothesis testing</li>
</ol>
<p>Currently, I see a few tools that are capable of doing this with varying levels of success and objectives. <a href=https://www.ycombinator.com/launches/J9T-structured-analyze-logs-with-ai>Structured</a> is a new company created to address this issue but <a href=https://neptune.ai/blog/machine-learning-approach-to-log-analytics>entrenched</a> players are also stepping in capture this market.</p>
<h2 id=search-problems>Search Problems<a hidden class=anchor aria-hidden=true href=#search-problems>#</a></h2>
<h3 id=query-optimizers>Query Optimizers<a hidden class=anchor aria-hidden=true href=#query-optimizers>#</a></h3>
<p>When queries aren&rsquo;t optimized correctly, it can introduce delays across the system because databases underpin a lot of systems. Solutions like <a href=https://www.airops.com/blog/using-ai-to-optimize-your-sql-queries>airops</a> and <a href=https://postgres.ai/products/joe>postgres.ai</a> already exist but they&rsquo;re not first class primitives. They&rsquo;re interfaces to the query optimizer, which <a href=https://www.vldb.org/pvldb/vol12/p1705-marcus.pdf>need to be tediously maintained, especially as the system’s execution and storage engines evolve</a>. There are benefits to understanding the history of calls to a database to exploit caches and improve index selection. What does a first class query optimizer primitive look like?</p>
<p><a href=https://www.vldb.org/pvldb/vol12/p1705-marcus.pdf>Neo</a> proposes a neural network based query optimizer that is comparable to optimizers from Microsoft and IBM. More importantly, it can dynamically adapt to <a href=https://www.vldb.org/pvldb/vol12/p1705-marcus.pdf>incoming queries, building upon its successes and learning from its failure</a>. Does this neural network fit the model of an system primitive? By adjusting the weights of this neural network, we can address novel situations based on the environment, storage needs, and traffic distribution.</p>
<p>System primitives can come in various shapes and formats - the invariance is that they can learn. I will add that they can also communicate and interact with an AI agent (section coming later).</p>
<h2 id=scheduling-problems>Scheduling Problems<a hidden class=anchor aria-hidden=true href=#scheduling-problems>#</a></h2>
<h3 id=schedulers>Schedulers<a hidden class=anchor aria-hidden=true href=#schedulers>#</a></h3>
<p>So far, we&rsquo;ve talked about creating first class system primitives for assignment and search problems. What about scheduling problems? What about scheduling and assignment problems? When you submit jobs to your cluster, you request system resources like compute and memory. But the system doesn&rsquo;t precisely know how much compute and memory you&rsquo;ll need in the duration of your program execution. What happens if there&rsquo;s a large burst of ML training jobs and you need to quickly provision compute and memory for those jobs? You could overprovision and waste resources or underprovision and redo or slow down the task. There are a few <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview>examples</a> that have already used AI to find a better packing solution.</p>
<p>We also see scheduling problems in process schedulers for <a href=https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-vsphere-cpu-sched-performance-white-paper.pdf>hyper-V</a>. We&rsquo;ve already seen RL agents handle generic scheduling and assignment problems and do considerably <a href=https://towardsdatascience.com/reinforcement-learning-for-production-scheduling-809db6923419>better</a> than conventional optimization techniques, suggesting that we could realize significant gains in many aspects of the system given how ubiquitous the scheduling+assignment problem is in systems.</p>
<h2 id=deployments>Deployments<a hidden class=anchor aria-hidden=true href=#deployments>#</a></h2>
<p>How do we deploy a system like this? How can we as humans interpret outcomes? How can we debug these solutions? It&rsquo;s not as straightforward to debug a declarative system. But there are more established methods to productionize AI models like <a href=https://www.claypot.ai/>Claypot</a> that at least create precedence for how to design these systems, ensure that we can detect model drift (among many other factors) and rollback when the model quality starts slipping.</p>
<hr>
<p>If you&rsquo;re interested, I&rsquo;m in the process of writing another blog post on why we need another compiler for distributed compute that <a href=http://www.cs.cmu.edu/~concert/papers/lics2004/symmetric.pdf>addresses both mobility of code and locality of resources</a>. I&rsquo;m not convinced that present compilers understand system requirements - they&rsquo;re locally optimized for the code but not globally optimized for the system. Since moving to the states, I&rsquo;ve noticed a general trend of fragmented systems: health, medical, finance, banking&mldr; There&rsquo;s a blog coming up on this too.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=next href=https://ankilp.github.io/posts/shortstories/>
<span class=title>Next Page »</span>
<br>
<span>Short stories</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on twitter" href="https://twitter.com/intent/tweet/?text=Anti-fragility%20in%20%28Distributed%29%20Systems&url=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f&title=Anti-fragility%20in%20%28Distributed%29%20Systems&summary=Anti-fragility%20in%20%28Distributed%29%20Systems&source=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f&title=Anti-fragility%20in%20%28Distributed%29%20Systems"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on whatsapp" href="https://api.whatsapp.com/send?text=Anti-fragility%20in%20%28Distributed%29%20Systems%20-%20https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Anti-fragility in (Distributed) Systems on telegram" href="https://telegram.me/share/url?text=Anti-fragility%20in%20%28Distributed%29%20Systems&url=https%3a%2f%2fankilp.github.io%2fposts%2fai-primitives%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2023 <a href=https://ankilp.github.io>Ankil Patel</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>